[{"authors":["admin"],"categories":null,"content":" This is Lee.\nI am a Ph.D. candidate in the Department of Psychological and Brain Sciences, affiliated with the Center for Information Technology and Society at UC Santa Barbara.\nI study social minds and perception using interdisciplinary methods, including behavioral measures, machine learning, and neuroimaging.\nI keep my visual arts separate. Visit my artist site here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/lee-q.-cui/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/lee-q.-cui/","section":"authors","summary":"This is Lee.\nI am a Ph.D. candidate in the Department of Psychological and Brain Sciences, affiliated with the Center for Information Technology and Society at UC Santa Barbara.","tags":null,"title":"Lee Q. Cui","type":"authors"},{"authors":null,"categories":null,"content":"Background AI systems increasingly make decisions with potential racial bias Prior research shows mixed perceptions of AI bias versus human bias This study tests emotional and cognitive responses to controlled bias vignettes Racial identity of perceivers tested as a moderating factor Methods N = 224 participants (52% female; 112 White, 112 non-White) recruited on Prolific Participants randomly assigned to 1 of 8 conditions crossing: Perpetrator type: Human (\u0026ldquo;Tod Smith\u0026rdquo;) vs AI (\u0026ldquo;TodSMith\u0026rdquo;) Bias explicitness: Blatant (explicit racial slurs) vs Subtle (neutral but unfair reason) Vignettes depict biased hiring decisions in varied professional roles Participants completed ratings on emotional upset, perceived racism, blame attribution (to perpetrator, trainer, society), and AI familiarity Additional personality and prejudice motivation measures administered Results Participants more upset and perceived blatant bias as more racist than subtle bias (p \u0026lt; .001) Humans blamed more than AI for bias, especially blatant bias (p \u0026lt; .01) Trainers blamed more when AI perpetrated bias (p \u0026lt; .001) No differences by participant race in upset or blame attribution Non-White participants reported greater resentment than White participants (p \u0026lt; .01) Reflection This research reveals that people hold humans more accountable than AI for biased behavior, particularly when bias is explicit. Meanwhile, blame shifts to AI trainers, showing awareness of AI’s dependence on human input. The findings deepen understanding of social perceptions of algorithmic fairness and bias, informing ethical AI development and public communication strategies.\n","date":1714176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714176000,"objectID":"33571d09d9fc59cd123bb808c7bf6a75","permalink":"/post/ai-dissertation/","publishdate":"2024-04-27T00:00:00Z","relpermalink":"/post/ai-dissertation/","section":"post","summary":"This project examines whether individuals judge racial bias differently when it is perpetrated by humans versus AI, how blatant versus subtle bias impacts perception, and whether perceiver race (White vs. non-White) moderates these effects. The study lays groundwork for further research on agency, intentionality, and outcome severity in bias evaluation.","tags":["Linear Mixed Models"],"title":"How do people perceive and respond to social bias from humans versus AI?","type":"post"},{"authors":null,"categories":null,"content":"Background Supreme Court rulings in 2013 and 2015 served as key case studies Twitter data from 2012 to 2017 provided a longitudinal, naturalistic dataset Tweets were filtered using hashtags such as #SCOTUS and #LoveWins Users were categorized based on profile cues (e.g., emojis, hashtags, bios) Only everyday users (under 1,000 followers) were included to reduce bias from influencers or institutions Methods Sentiment Analysis: Used BERT, a deep-learning NLP model, to analyze the emotional tone of tweets Categorization: Grouped users into LGBTQ+ and non-LGBTQ+ based on profile indicators Modeling: Employed linear mixed-effects models (REML) to analyze sentiment across months, focusing on the years 2013 and 2015 Included random intercepts and slopes by month to capture variability in sentiment Results Contrary to predictions, LGBTQ+ tweets were more positive on average than non-LGBTQ+ tweets in both 2013 and 2015 2013 Fixed Effect: B = 0.152, SE = 0.004 2015 Fixed Effect: B = 0.156, SE = 0.006 Random Effects (2015): June baseline sentiment (Intercept = 0.0029) and LGBTQ+ slope (0.0139) indicate elevated positivity during the ruling month Seasonal trends observed, including a sentiment dip in 2016 (possibly linked to Trump’s election) Reflection This study challenges assumptions about online sentiment as a direct reflection of lived disadvantage. LGBTQ+ users may express higher positivity due to community resilience, social norms around self-presentation, or differing patterns of social media engagement. It also raises questions about the role of digital spaces in coping with structural stigma and highlights the complexity of interpreting sentiment in marginalized groups.\nFuture directions include deeper subgroup analysis (e.g., gay vs. lesbian users), examining non-verbal cues (e.g., emoji use), and integrating offline mental health or well-being indicators where possible.\n","date":1651017600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651017600,"objectID":"1d92526beef2c467e2fa54be5d4f7a77","permalink":"/post/twitter/","publishdate":"2022-04-27T00:00:00Z","relpermalink":"/post/twitter/","section":"post","summary":"Supported by advanced NLP tools and statistical modeling, this project investigates both individual- and structural-level stigma and its interaction with online behavior and societal norms.","tags":["Sentiment Analysis","NLP","Linear Mixed Models"],"title":"How do government rulings shape public discourse and the emotional well-being of stigmatized communities?","type":"post"},{"authors":null,"categories":null,"content":"How do human social biases shape algorithmic decision-making? While face classification models are often treated as neutral, their outputs are deeply influenced by biased human input. My dissertation explores how racial prejudice—encoded through dataset construction and annotation—drives inequality in AI-based face classification systems.\nFunded and supported through multiple research grants, this project sits at the intersection of psychology, data science, and ethics. I investigated how racial diversity in datasets and annotator bias influence a model\u0026rsquo;s performance and decisions—especially for racially ambiguous faces.\nBackground 2 large-scale studies: one focused on dataset diversity, the other on annotator bias Trained face classification models using real and synthetic facial datasets Collected demographic annotations from participants with varying racial prejudice scores Measured downstream effects on model performance, especially accuracy by race and ambiguity Methods Constructed racially diverse and imbalanced training datasets Collected annotator ratings alongside implicit and explicit bias measures (e.g., IAT) Trained models in Python using standard deep learning pipelines Analyzed data using R Studio and Python (e.g., accuracy metrics, regression, SEM) Conducted behavioral validation using perception studies of model outputs Results Models trained on imbalanced datasets underperformed on underrepresented racial groups Annotator prejudice significantly influenced labeling, which distorted model accuracy—especially for ambiguous faces Racial ambiguity was often misclassified in ways that mirrored annotator bias Human input—not model architecture—was the dominant source of inequality Reflection This project deepened my interest in responsible AI and human-centered research design. It showed me how critical it is to understand social perception and psychological bias when working with real-world data and automated decision systems. I hope to apply these insights to improve transparency, fairness, and inclusivity in future research and UX strategy work.\nTags:\nAI Bias · Human-AI Interaction · UX Research · Ethical AI · Dataset Design · Psychological Measurement · Algorithmic Fairness\n","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"4fafa987888ff8a69bb918fcbce8e614","permalink":"/post/ai-face/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/post/ai-face/","section":"post","summary":"This project explores how human bias in dataset selection and annotation fuels racial inequality in AI-based face classification systems.","tags":["AI"],"title":"How do human social biases shape algorithmic decision-making?","type":"post"},{"authors":null,"categories":null,"content":"Background Most widely used erotic stimulus sets are Western-centric Attitudes toward sex tend to be more conservative in East Asian cultures Existing datasets lack adequate East Asian representation in erotic imagery This research aimed to create a new validated dataset tailored for use in East Asian sexuality research Methods Collected 237 erotic and 108 control images across 6 categories: Dressed males (44), Semi-nude males (65), Nude males (64) Dressed females (64), Semi-nude females (52), Nude females (56) Participants: 40 heterosexual East Asian adults (20 men, 20 women) Participants rated opposite-sex images on: Sexual arousal Pleasantness Sexual attractiveness Ratings analyzed by gender and stimulus category Results Males reported highest arousal, pleasantness, and attractiveness for nude female images Females reported highest arousal and positive responses for semi-nude male images Gender differences were consistent across categories Ratings confirmed clear patterns of gender-specific preferences in erotic visual processing Reflection This project contributes a validated, culturally sensitive erotic stimuli set for use in psychological and sexuality research involving East Asian populations. It underscores how culture and gender shape the perception of erotic content, with implications for cross-cultural studies in sexual function, emotion, and arousal. I’m especially interested in how this work intersects with broader themes in affective science, media perception, and cultural psychology. fferences by participant race in upset or blame attribution\nNon-White participants reported greater resentment than White participants (p \u0026lt; .01) Reflection This research reveals that people hold humans more accountable than AI for biased behavior, particularly when bias is explicit. Meanwhile, blame shifts to AI trainers, showing awareness of AI’s dependence on human input. The findings deepen understanding of social perceptions of algorithmic fairness and bias, informing ethical AI development and public communication strategies.\n","date":1556323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556323200,"objectID":"965ff7d020354a89498eac71ac7e3ace","permalink":"/post/erosinm-database/","publishdate":"2019-04-27T00:00:00Z","relpermalink":"/post/erosinm-database/","section":"post","summary":"Mainstream erotic image datasets underrepresent East Asian figures, limiting research validity for East Asian populations. This project introduces a normed erotic picture dataset specifically designed for East Asian cultural contexts, incorporating self-reported arousal, pleasantness, and sexual attractiveness ratings from heterosexual men and women.","tags":["Sentiment Analysis","NLP","Linear Mixed Models"],"title":"How do cultural attitudes and gender shape sexual arousal responses?","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8e7bc052bdfc6746ea2bb6595e8093eb","permalink":"/home/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]